{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp logtools \n",
    "# default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# not a dep but we need it for show_doc\n",
    "from nbdev.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `logtools` Log important information for debugging and meta-analysis.\n",
    "\n",
    "> These functions log internal actions taken when interacting with data sources and make information available in an easily acessible way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average user won't have to interact with many of these functions other than (`logfile_to_df()`) directly during their interactions with the high level API, however many of these methods exist and support `DatabaseConnection`, `DBConnector` & `S3Bucket`.\n",
    "\n",
    "However, knowing the difference between `FileLog` and `StreamLog` can be very helpful. \n",
    "\n",
    "- For scripts and processes you wish to re-run and which have many queries running, we recommend using something like `FileLog('/tmp/myprocess.log')` so that a user can read this file. Additionally, the `logfile_to_df()` function can be used on pre-existing logfiles to analyze your processes performance. \n",
    "\n",
    "- For simple ad-hoc tasks where you want immediate feedback printed to STDOUT, we recommend `StreamLog`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "LOGNAME = \"analytics_toolbox\"\n",
    "\n",
    "class BaseLog:\n",
    "    def _init_logobj(self, handler):\n",
    "        self.logger = logging.getLogger(LOGNAME)\n",
    "        self.logger.setLevel(logging.DEBUG)\n",
    "        self.fh = handler\n",
    "        self.fh.setLevel(logging.DEBUG)\n",
    "        \n",
    "    def debug(self, message):\n",
    "        self.logger.debug(message)\n",
    "\n",
    "    def info(self, message):\n",
    "        self.logger.info(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "class FileLog(BaseLog):\n",
    "    \"\"\"\n",
    "    Creates a local logfile where these classes are imported,\n",
    "    showing database connections and what was run on databases.\n",
    "    \"\"\"\n",
    "    def __init__(self, logfile):\n",
    "        handler = logging.FileHandler(logfile)\n",
    "        self._init_logobj(handler)\n",
    "        \n",
    "class LocalLog(FileLog):\n",
    "    def __init__(self, logfile):\n",
    "        super().__init__(logfile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class StreamLog(BaseLog):\n",
    "    \"\"\"\n",
    "    Logs to STDOUT, useful for scripts that query DBs.\n",
    "    Every query will print JSON with information about SQL ran. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        handler = logging.StreamHandler(sys.stdout)\n",
    "        self._init_logobj(handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "def logfile_to_df(logfile):\n",
    "    pass "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
